% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage{indentfirst}
\usepackage{geometry} % to change the page dimensions
\geometry{letterpaper, left=25mm, right=25mm, top=25mm, bottom=25mm}
\usepackage[pdftex]{graphicx} % support the \includegraphics command and options

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{multirow}
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{epsfig}
\usepackage{pdflscape}
\usepackage{longtable}
%\usepackage[pdftex]{graphicx}


%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{plain} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}
\usepackage{kotex}
\usepackage[overload]{empheq}
\usepackage{amsthm, amssymb, amsmath, amsfonts, amsxtra}
\usepackage{color}
\usepackage[stable]{footmisc}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

\usepackage{mathrsfs}
\usepackage{mdframed}
\usepackage{float}

%% For fancy fonts
%\usepackage[lf]{Baskervaldx} % lining figures
%\usepackage[bigdelims,vvarbb]{newtxmath} % math italic letters from Nimbus Roman
%\usepackage[cal=boondoxo]{mathalfa} % mathcal from STIX, unslanted a bit

\usepackage{bm}
\usepackage{bbm}
%% Times new roman 
%\usepackage{newtxtext,newtxmath}


%% Spacing
\usepackage{setspace}

%% HREF
\usepackage[unicode=true, pdfusetitle,
bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
{hyperref}
\hypersetup{
	colorlinks = true,
	urlcolor = blue,
	linkcolor = red,
	citecolor = blue,
	%  pdfauthor = {\name},
	%  pdfkeywords = {economics, econometrics,
	% industrial organization,
	%    applied microeconomics},
	%  pdftitle = {\name: Curriculum Vitae},
	%  pdfsubject = {Curriculum Vitae},
	%  pdfpagemode = UseNone
}

%% TIKZ for figure drawing
\usepackage{tikz}

%% BIBTEX
\usepackage{natbib}
\usepackage{authblk}

%% EPS figure
\usepackage{epstopdf}

%% Import package
\usepackage{import}

%Here I define some theorem styles and shortcut commands for symbols I use often
\theoremstyle{definition}

\newmdtheoremenv{defn}{Definition}
\numberwithin{defn}{subsection}
\newmdtheoremenv{thm}{Theorem}
\numberwithin{thm}{subsection}
\newmdtheoremenv{coro}[thm]{Corollary}
\newmdtheoremenv{claim}[thm]{Claim}
\newtheorem*{claim*}{Claim}
\newtheorem*{summ}{Summary}
\newtheorem*{rmk}{Remark}
\newmdtheoremenv{lemma}[thm]{Lemma}
\newtheorem*{joke}{Joke}
\newtheorem{ex}{Example}
\numberwithin{ex}{subsection}
\newtheorem*{soln}{Solution}
\newmdtheoremenv{prop}[thm]{Proposition}
\newmdtheoremenv{assm}[thm]{Assumption}

\newcommand{\lra}{\longrightarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\surj}{\twoheadrightarrow}
\newcommand{\graph}{\mathrm{graph}}
\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\Z}{\bb{Z}}
\newcommand{\Q}{\bb{Q}}
\newcommand{\R}{\bb{R}}
\newcommand{\N}{\bb{N}}
\newcommand{\M}{\mathbf{M}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\MM}{\mathscr{M}}
\newcommand{\HH}{\mathscr{H}}
\newcommand{\Om}{\Omega}
\newcommand{\Ho}{\in\HH(\Om)}
\newcommand{\bd}{\partial}
\newcommand{\del}{\partial}
\newcommand{\bardel}{\overline\partial}
\newcommand{\textdf}[1]{\textbf{\textsf{#1}}\index{#1}}
\newcommand{\img}{\mathrm{img}}
\newcommand{\ip}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\inter}[1]{\mathrm{int}{#1}}
\newcommand{\exter}[1]{\mathrm{ext}{#1}}
\newcommand{\cl}[1]{\mathrm{cl}{#1}}
\newcommand{\ds}{\displaystyle}
\newcommand{\vol}{\mathrm{vol}}
\newcommand{\cnt}{\mathrm{ct}}
\newcommand{\osc}{\mathrm{osc}}
\newcommand{\LL}{\mathbf{L}}
\newcommand{\UU}{\mathbf{U}}
\newcommand{\support}{\mathrm{support}}
\newcommand{\AND}{\;\wedge\;}
\newcommand{\OR}{\;\vee\;}
\newcommand{\Oset}{\varnothing}
\newcommand{\st}{\ni}
\newcommand{\wh}{\widehat}
\newcommand{\plim}{\overset{p}{\rightarrow}}
\newcommand{\dlim}{\overset{d}{\rightarrow}}
\newcommand{\sumi}{\sum_{i=1}^n}
\newcommand{\ninfty}{n\rightarrow\infty}
\newcommand{\bhat}{\hat{\beta}}
\newcommand{\yhat}{\hat{y}}
\newcommand{\ehat}{\hat{e}}
\newcommand{\xbar}{\bar{x}}
\newcommand{\ybar}{\bar{y}}
\newcommand{\zbar}{\bar{z}}
\newcommand{\iid}{\overset{i.i.d}{\sim}}
\newcommand{\thetah}{\hat{\theta}}
\newcommand{\Vhat}{\hat{V}}
\newcommand{\btilde}{\tilde{\beta}}
\newcommand{\bmd}{\btilde_{md}}
\newcommand{\bgmm}{\bhat_{GMM}}
\newcommand{\ow}{\text{ otherwise }}
\newcommand{\textif}{\text{ if }}

\DeclareMathOperator*{\amax}{arg\,max}
\DeclareMathOperator*{\amin}{arg\,min}

%Pagination stuff.
\pagenumbering{arabic}

%%% The "real" document content comes below...

\title{Introduction to Econometrics II\thanks{Columbia University Economics Ph.D. First Year 2018-2019. Professor Jushan Bai and Simon Lee. Mostly from Professors' lectures, \cite{hanseneconometrics}, \cite{cameronandtrivedi}, \cite{mostlyharmless}, \cite{wooldridgepanel},  and sometimes Paul Koh's note from last year and Professor Christoph Rothe's lecture notes from 2017 Spring.}} 
\author{Dong Woo Hahm\thanks{Department of Economics, Columbia University. \href{mailto:dongwoo.hahm@columbia.edu}{dongwoo.hahm@columbia.edu}. \\Please email me for any error. This note will be continuously updated throughout the semester. Please do not circulate outside of the class.}}
\date{Spring 2019} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 
		
\begin{document}
\maketitle
\tableofcontents
\clearpage
%\abstract{}
\section{Review of Asymptotic Theory}
\noindent Usually econometrics is about analyzing data from an economic context and has steps including:
\begin{itemize}
\item Formulating an appropriate model
\item Computing estimates of unknown parameters in such a model
\item Quantifying the uncertainty about those estimates
\item Use these measures of uncertainty to draw empirical conclusions
\end{itemize}
Asymptotic theory is usually related to the third step, ``Quantifying the uncertainty".
\subsection{Basic Setup}
\begin{itemize}
	\item Data : $\{z_i \}_{i=1}^n$ with joint distribution $P_n$
	\item Model : A set $\mathcal{P}_n$ of potential candidates for $P_n$. Restrictions may be imposed as necessary.
	\item Independent and identically distributed (i.i.d) data
	\begin{align*}
	P_n=P\times P\cdots \times P
	\end{align*}
	\item Random variable (vector) of interest:
	\begin{align*}
	\thetah_n=f_n(z_1,\cdots,z_n)
	\end{align*}
	e.g) estimator, test statistics ...
\end{itemize}
We are interested in features of the distribution of $\thetah_n$ with a finite sample size $n$, which is usually impossible or impractical.
%\begin{ex}\leavevmode
%	\begin{itemize}
%		\item $z_i\iid N(\mu,\sigma^2)$ : $\thetah_n=\bar{z}\sim N(\mu,\sigma^2/n)$
%		\item $z_i\iid N(\mu,\sigma^2)$ : $\thetah_n=\sqrt{n}(\bar{z}-\mu)/s_z\sim t_{n-1}$
%		\item $z_i\iid Exp(\lambda)$ : $\thetah_n=\bar{z}$. The distribution of $\thetah_n$ is known in principle but without convenient closed form expression.
%		\item $z_i\iid (\mu,\sigma^2)$ : $\thetah_n=\bar{z}$. Only first two moments are known and we don't know about the exact distribution.
%	\end{itemize}
%\end{ex}
Instead, we typically use asymptotics to derive approximations to the distribution of $\thetah_n$. The general idea is to think of $\thetah_n$ as the $n$th element of an infinite sequence and to calculate the limit of the sequence (if exists).% We have two things we can choose : the \textbf{sequence} itself and the \textbf{type of the limit}. We will first focus on the type of limits (holding $P$ fixed and increasing $n$ to infinity) and then return to alternative concepts later.

\subsection{Modes of Convergence}
\subsubsection{Covergence in Probability}
\begin{defn}
	A sequence of random variables $z_n$ is said to \textbf{converge in probability} to a random variable $z$ if for any $\delta>0$, $\lim_{n\rightarrow\infty}P(|z_n-z|\le\delta)=1$ or equivalently, $\lim_{n\rightarrow\infty}P(|z_n-z|>\delta)=0$ and we denote as $z_n\overset{p}{\rightarrow}z$ or $p\lim_{n\rightarrow\infty}z_n=z$.
\end{defn}

\begin{thm}\label{thm:wlln}
	\textbf{(Khintchine's Weak Law of Large Numbers)}\\
	If $z_1,\cdots z_n$ are i.i.d with $E(z_i)=\mu$, then $\bar{z}_n\overset{p}{\rightarrow}\mu$ where $|\mu|<+\infty$.
	\begin{proof}
		Assume $Var(z_i)=\sigma^2<+\infty$. For any $\delta>0$,
		\begin{align*}
		P(|\bar{z}_n-\mu|>\delta)&\le\frac{E(|\bar{z}_n-\mu|^2)}{\delta^2}\tag*{: Chebyshev's Inequality}\\
		&=\frac{\sigma^2}{\delta^2\cdot n}\\
		&\rightarrow 0\tag*{as $n\rightarrow\infty$}
		\end{align*}
	\end{proof}
\end{thm}

\subsubsection{Convergence in Distribution}
\begin{defn}
	Let $z_n$ be a sequence of random variables and define $F_n(x)=P(z_n\le x)$. Let $z$ a random variable of which distribution function is $F(x)=P(z\le x)$. $z_n$ is said to \textbf{converge in distribution} to $z$ if $F_n(x)\rightarrow F(x)$ as $n\rightarrow\infty$ at all $x$ where $F$ is continuous. We denote this by $z_n\overset{d}{\rightarrow}z$.
\end{defn}

\begin{itemize}
	\item $z$ is usually called the asymptotic distribution of limit distribution of $z_n$.
	\item $z_n\overset{d}{\rightarrow}z$ does not necessarily mean that $z_n$ and $z$ are close (only the cdfs are close) where as $z_n\overset{p}{\rightarrow}z$ implies $z_n$ and $z$ are close.
	\item $z_n\overset{p}{\rightarrow}z$ implies $z_n\overset{d}{\rightarrow}z$
	\item However, $z_n\dlim z$ don't imply $z_n\plim z$\\
	For example, consider $Y_n$ and $Y$ that are mutually independent with distributions given by
	\begin{align*}
	Y_n=\begin{cases}
	1\text{ with probability }\frac{1}{2}+\frac{1}{n+1}\\
	0\text{ with probability }\frac{1}{2}-\frac{1}{n+1}
	\end{cases}\\
	Y=\begin{cases}
	1\text{ with probability }\frac{1}{2}\\
	0\text{ with probability }\frac{1}{2}
	\end{cases}
	\end{align*}
	\item If $z=c\in\R(\Leftrightarrow P(z=c)=1)$, that is the limit distribution $z$ is degenerate, then $z_n\overset{p}{\rightarrow}c$ $\Leftrightarrow$ $z_n\overset{d}{\rightarrow}c$
\end{itemize}

\begin{thm}\label{thm:lindeberglevyclt}
	\textbf{(Lindeberg-Levy Central Limit Thoerem)}\\
	Let $z_1,\cdots z_n$ be i.i.d with $E(z_i)=\mu,Var(z_i)=\sigma^2$ where $|\mu|<+\infty,\sigma^2<+\infty$. Then
	\begin{align*}
	\sqrt{n}(\bar{z}_n-\mu)=\frac{1}{\sqrt{n}}\sum_{i=1}^n (z_i-\mu)\overset{d}{\rightarrow} N(0,\sigma^2)
	\end{align*}
	That is,
	\begin{align*}
	\lim_{n\rightarrow\infty}P(\frac{1}{\sqrt{n}}\sum_{i=1}^n (z_i-\mu)\le a)=\int_{-\infty}^a\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2\sigma^2}x^2)dx.
	\end{align*}
	for all $a\in\R$.
\end{thm}

\begin{rmk}\leavevmode
	\begin{itemize}
		\item WLLN and CLT :\\
		By WLLN, $\bar{X}-\mu\plim 0$. In some sense with WLLN, $\bar{X}-\mu$ goes to 0 too fast as $\ninfty$. Thus by multiplying $\sqrt{n}$, the CLT slows down the speed of convergence to make it converge to some non degenerate distribution, i.e, $\sqrt{n}(\bar{X}-\mu)\dlim Z\sim N(0,\sigma^2)$.
		\item It is often useful to know if $Z\sim N(0,\sigma^2)$,
		\begin{align*}
		E(X^p)=\begin{cases}
		0&\text{if }p\text{ is odd}\\
		\sigma^p(p-1)!!&\text{if }p\text{ is even}
		\end{cases}
		\end{align*}
		where $n!!=\prod_{k=0}^{\lceil\frac{n}{2}\rceil-1}(n-2k)=n(n-2)\cdots 1 $ is double factorial.
	\end{itemize}
\end{rmk}


%Berry-Esseen Theorem tells us how good the approximation provided by the CLT.
%\begin{thm}
%	\textbf{(Berry-Esseen)}\\
%	For all sample sizes $n$ and all $t\in\R$,
%	\begin{align*}
%	\Bigr|P\Bigr(\sqrt{n}\frac{\bar{z}-\mu}{\sigma}\le t\Bigr)-\Phi(t)\Bigr|<\frac{C\cdot E(|z_i-\mu|^3)}{\sigma^3\sqrt{n}}
%	\end{align*}
%	for some constant $C\in (0.4097,0.4748)$.
%\end{thm}
%Berry-Esseen Theorem says the accuracy of CLT approximation depends on the standardized absolute 3rd moment of the distribution of $z_i$ and the square root of the sample size, $\sqrt{n}$. Thus approximating distributions of $z_i$ with heavy tails (for example, binary random variables) using CLT might have poor accuracy.

Similar things hold for random vectors, a vector of random variables. First, let's define random vectors.

\begin{defn}
	$\bm{y}=\begin{pmatrix}y_1\\\vdots\\y_m\end{pmatrix}\in\R^m$ is called a \textbf{random vector}. We define $E(\bm{y})=\begin{pmatrix}E(y_1)\\\vdots\\E(y_m)\end{pmatrix}\in\R^m$ and $||\bm{y}||=\sqrt{\bm{y}^T\bm{y}}=(y_1^2+\cdots y_m^2)^{\frac{1}{2}}$.
\end{defn}

\begin{thm}
	$E||\bm{y}||<+\infty\Leftrightarrow E|y_j|<+\infty,\forall j=1,2,\cdots,m$ where $m$ is a finite natural number.
	\begin{proof}
		$(\impliedby)$ $y_1^2+\cdots+y_m^2\le(|y_1|+\cdots+|y_m|)^2$ $\Rightarrow||y||\le|y_1|+\cdots+|y_m|$. So from $E|y_i|<+\infty,E||y||<+\infty$.\\
		$(\implies)$ $|y_j|\le(|y_1|^2+\cdots+|y_m|^2)^{\frac{1}{2}},\forall j=1,2,\cdots,m$ $\Rightarrow|y_j|\le||\bm{y}||,\forall j=1,2,\cdots,m$. So from $E||\bm{y}||<+\infty,E|y_j|<+\infty,\forall j=1,2,\cdots,m$.
	\end{proof}
\end{thm}

Convergence in probability of a random vector is defined as convergence in probability of all elements in the vector. Hence, multivariate version of WLLN (Theorem \ref{thm:wlln}) holds.

\begin{thm}\label{thm:wlln(vector)}
	\textbf{(WLLN for Random Vectors)}\\
	Let $\bm{y}_i$ be i.i.d where $\bm{y}_i\in\R^m$ s.t. $E||\bm{y}_i||<+\infty,\forall i$. Let $\bar{\bm{y}}_n=\frac{1}{n}\sum_{i=1}^n \bm{y}_i=\begin{pmatrix}
	\bar{y}_1\\\vdots\\\bar{y}_m\end{pmatrix}$, then 
	\begin{align*}
	\bar{\bm{y}}_n\overset{p}{\rightarrow}E(\bm{y}_i)=\begin{pmatrix}E(y_{i1})\\\vdots\\E(y_{im})\end{pmatrix}
	\end{align*}
	\begin{proof}
		By definition, $\bar{\bm{y}}_n\plim E(\bm{y}_i)$ if and only if $\bar{y}_j\plim \mu_j,\forall j=1,2,\cdots,m$. The latter holds if $E|y_j|<+\infty,\forall j=1,2,\cdots,m$ which is equivalent to $E|\bm{y}_i|<+\infty$.
	\end{proof}
\end{thm}

Though Lindeberg-Levy CLT (Theorem \ref{thm:lindeberglevyclt}) only provides the case for scalar random variables, it can be extended to multivariate data via Cramer-Wold device.

\begin{thm}\label{thm:cramerwold}
	\textbf{(Cramer-Wold)}	\\
	Let $\hat{\gamma}_n,\gamma_{\infty}$ be vector-valued random vectors. Then $\hat{\gamma}_n\dlim \gamma_{\infty}$ if and only if $\lambda'\hat{\gamma}_n\dlim\lambda'\gamma_{\infty}$ for all fixed vectors $\lambda$ with $\lambda'\lambda=1$.\footnote{The last condition that $\lambda'\lambda=1$ is often omitted in some versions, and it's not necessary.}
\end{thm}

%One implication of Cramer-Wold is that if $\{z_i \}_{i=1}^n$ is i.i.d with mean $\mu$ and variance $\Sigma$ then
%\begin{align*}
%\sqrt{n}(\bar{z}-\mu)\dlim N(0,\Sigma)
%\end{align*}
%which yields the approximation that $\bar{z}\sim^a N(\mu,\Sigma/n)$.

\begin{thm}\label{thm:clt(vector)}
	\textbf{(Multivariate Lindeberg-Levy Central Limit Theorem)}	\\
	If $\bm{y}_i\in\R^k$ are independent and identically distributed and $E||\bm{y}_i||^2<+\infty$, then as $\ninfty$
	\begin{align*}
	\sqrt{n}(\bar{\bm{y}}_n-\mu)\dlim N(0,V)
	\end{align*}
	where $\mu=E(\bm{y})$ and $V=E((\bm{y}-\mu)(\bm{y}-\mu)')$.
	\begin{proof}
		Fix some $\lambda\in \R^k$ such that $\lambda'\lambda=1$. Define $u_i=\lambda'(\bm{y}_i-\mu)$. Then $u_i$ are i.i.d with $E(u_i^2)=\lambda'V\lambda<\infty$. We have
		\begin{align*}
		\lambda'\sqrt{n}(\bar{\bm{y}_n}-\mu)=\frac{1}{\sqrt{n}}\sumi u_i\dlim N(0,\lambda'V\lambda)
		\end{align*}
		If some random vector $\bm{z}\sim N(0,V)$ then $\lambda' \bm{z}\sim N(0,\lambda'V\lambda)$. Thus, we have
		\begin{align*}
		\lambda'\sqrt{n}(\bar{\bm{y}}_n-\mu)\dlim \lambda' \bm{z}
		\end{align*}
		Since the choice of $\lambda$ was arbitrary, by Cramer-Wold device (Theorem \ref{thm:cramerwold}), we have
		\begin{align*}
		\sqrt{n}(\bar{\bm{y}}_n-\mu)\dlim\bm{z}
		\end{align*}
	\end{proof}
\end{thm}
\begin{rmk}
	Note that having convergence in distribution to a normal distribution in each component does not imply the random vector jointly converge to joint normal. Hence for asymptotic ``joint" normality results, you should make use of multivariate CLT.\footnote{Recall Pepe's PS2 Q3, PS3 Q4?}
\end{rmk}

\subsubsection{Sufficient Conditions for Joint Weak Convergence}
For probability limits, it is straightforward.
\begin{thm}
	If $X_n\plim X$ and $Y_n\plim Y$ then $(X_n,Y_n)\plim (X,Y)$	
\end{thm}
However for weak convergence, things are a bit more complicated. The following two theorems give some examples of sufficient conditions for joint weak convergence.
\begin{thm}
	If $X$ is independent of each $Y_i$ in $\{Y_i\}$ and $Y_n\dlim Y$, then $(X,Y_n)\dlim (X,Y)$.
\end{thm}

\begin{thm}
	If $X_n,Y_n$ and $X,Y$ are mutually independent, then the marginal convergence in distribution implies joint convergence in distribution.
	\begin{align*}
	X_n\dlim X, Y_n\dlim Y \Rightarrow (X_n,Y_n)\dlim (X,Y)
	\end{align*}
	\begin{proof}
		\begin{align*}
		\lim_{\ninfty}P(X_n\le x,Y_n\le y)&=\lim_{\ninfty}P(X_n\le x)P(Y_n\le y)\\
		&=\lim_{\ninfty}P(X_n\le x)\lim_{\ninfty}P(Y_n\le y)\\
		&=P(X\le x)P(Y\le y)\\
		&=P(X\le x,Y\le y)
		\end{align*}
	\end{proof}
\end{thm}

\subsection{Continuous Mapping Theorem and Delta Method}
Continuous Mapping theorem and Slutsky's theorem tell us how to manipulate limits in probability and distribution.

\begin{thm}\label{thm:CMT}
	\textbf{(Continuous Mapping Theorem)}\\
	If $z_n\overset{p,d}{\rightarrow}z$, and $g(\cdot)$ is has the set of discontinuity points $D_g$ such that $Pr(z\in D_g)=0$, then $g(z_n)\overset{p,d}{\rightarrow}g(z)$.\footnote{Note that if $z$ is a degenerate point, the condition reduces to $g(\cdot)$ is continuous at $z$.}
	\begin{proof}
		I only prove when $z=c$ is degenerate.
		
		Since $g$ is continuous at $c$, we can find a $\delta>0$ such that if $||z_n-c||<\delta$ then $||g(z_n)-g(c)||\le\epsilon$ for all $\epsilon>0$. Recall that $A\subset B$ implies $P(A)\le P(B)$. Thus $P(||g(z_n)-g(c)||\le\epsilon)\ge P(||z_n-c||<\delta)\rightarrow1$ as $n\rightarrow\infty$ by the assumption that $z_n\overset{p}{\rightarrow}c$. Hence $g(z_n)\overset{p}{\rightarrow}g(c)$ as $n\rightarrow\infty$.
	\end{proof}
\end{thm}

\begin{thm}\label{thm:slutsky}
	\textbf{(Slutsky's Theorem)}\\
	If $X_n\overset{d}{\rightarrow}Z,Y_n\overset{p}{\rightarrow}c$ as $n\rightarrow\infty$, then
	\begin{enumerate}
		\item $X_n+Y_n\overset{d}{\rightarrow}Z+c$
		\item $X_n-Y_n\dlim Z-c$
		\item $X_nY_n\overset{d}{\rightarrow}Zc$
		\item $\frac{X_n}{Y_n}\overset{d}{\rightarrow}\frac{Z}{c}$ if $c\neq0$
	\end{enumerate}
\end{thm}

Delta method is another way of approximating the distribution of ``smooth" transformations of simpler objects.%\footnote{The method used in proof of Delta method using Taylor expansion is extremely useful in econometrics. We will get to see this later in course for example in proving asymptotic normality of any type of extremum estimators, LM tests, or LR tests.}

\begin{thm}\label{thm:deltamethod}
	\textbf{(Delta Method)}\\
	Suppose that $\sqrt{n}(\hat{\theta}_n-\theta_0)\dlim\xi\in\R ^m$ where $g:\R^m\rightarrow\R^k,k\le m$ is continuously differentiable at $x=\theta_0$. Then $\sqrt{n}(g(\hat{\theta}_n)-g(\theta_0))\dlim G^T\xi$ where $G=(g'(\theta_0))^T$, the transpose of Jacobian matrix of $g$ evaluated at $\theta_0$.
	\begin{proof}
		Prove only for the case $m=k=1$.\\
		$g(x)=g(\theta_0)+g'(\bar{x})(x-\theta_0)$ where $\bar{x}$ is in between $x$ and $\theta_0$.\\
		Replace $x=\hat{\theta}_n$ then $g(\hat{\theta}_n)=g(\theta_0)+g'(\bar{\theta}_n)(\hat{\theta}_n-\theta_0)$ where $\bar{\theta}_n$ is in between $\hat{\theta}_n$ and $\theta_0$. Thus,
		\begin{align*}
		\sqrt{n}(g(\hat{\theta}_n)-g(\theta_0))=g'(\bar{\theta}_n)\sqrt{n}(\hat{\theta}_n-\theta_0)
		\end{align*}
		Since $\hat{\theta}_n\plim\theta_0$ and $\sqrt{n}(\hat{\theta}_n-\theta_0)\dlim\xi$ and $g'$ is continuous at $\theta_0$ and $||\bar{\theta}_n-\theta_0||\le||\hat{\theta}_n-\theta_0||,g'(\bar{\theta}_n)\plim g'(\theta_0)$.\\
		Thus, $\sqrt{n}(g(\hat{\theta}_n)-g(\theta_0))\dlim g'(\theta_0)\xi$ by Slutsky's theorem.
	\end{proof}	
	Note that we implicitly require that $G$ is full-rank.
\end{thm}

\subsection{Stochastic Orders}


\begin{defn}\textbf{(Stochastic Orders)}\\
	For deterministic sequences,
	\begin{itemize}
		\item $x_n=o(1)\iff x_n\rightarrow0$
		\item $x_n=o(a_n)\iff \frac{x_n}{a_n}\rightarrow 0$
		\item $x_n=O(1) \iff \exists M<+\infty$ s.t. $|x_n|\le M,\forall n$
		\item $x_n=O(a_n) \iff \frac{x_n}{a_n}=O(1)$
	\end{itemize}
	For stochastic sequences, 
	\begin{itemize}
		\item $z_n=o_p(1)\iff z_n\plim 0$
		\item $z_n=o_p(a_n)\iff\frac{z_n}{a_n}\plim 0$
		\item $z_n=O_p(1)\iff \forall\epsilon>0,\exists M_{\epsilon}>0$ s.t. $P(|z_n|>M_{\epsilon})<\epsilon,\forall n$\footnote{$z_n=O_p(1)$ is equivalent to saying that $z_n$ is stochastically bounded which roughly means that the tail probability is small.}
		\item $z_n=O_p(a_n)\iff\frac{z_n}{a_n}=O_p(1)$
%		\item $a_n$ is a deterministic sequence but we allow it to be a sequence of random variables as well.
	\end{itemize}
%	\begin{rmk}
%		\textbf{($O_p(1)$ and Uniformly Tight)}
%		\begin{itemize} 
%			\item $X_n=O_p(1)$ means ${X_n}$, a sequence of random variables is \textbf{uniformly tight}. 
%			\item Any random variable $X$ itself is a $O_p(1)$ in the sense that a sequence of random variables $\{X,X,X,X\cdots\}$ is uniformly tight.
%		\end{itemize}
%	\end{rmk}
\end{defn}


\begin{ex} \leavevmode
	\begin{itemize}
		\item For any consistent estimator $\hat{\beta}$ for $\beta$, $\hat{\beta}=\beta+o_p(1)$.
		\item For $\bhat$ such that $\sqrt{n}(\bhat-\beta)\dlim N(\cdot,\cdot)$, $\hat{\beta}=\beta+O_p(n^{-1/2}).$
	\end{itemize}
\end{ex}

\begin{rmk}
	\textbf{(Some Properties of Stochastic Orders)}
	\begin{enumerate}
		\item $z_n=O_p(a_n)\Rightarrow z_n=o_p(b_n)$ for any $b_n$ such that $a_n/b_n\rightarrow 0$.
		\item $z_n=o_p(1)\Rightarrow z_n=O_p(1)$
		\item $z_n=o_p(1)\nLeftarrow z_n=O_p(1)$
		\item $z_n\plim z \Rightarrow z_n=O_p(1)$
		\begin{proof}
			Fix $\epsilon>0.$
			\begin{itemize}
				\item $\exists M(\epsilon)>0$ s.t. $P(|z|>\frac{M(\epsilon)}{2})<\frac{\epsilon}{2}$ since $P(|z|>t)=F_z(-t)+(1-F_z(t))\rightarrow 0$.
				\item Since $z_n\plim z$, $\lim_n P(|z_n-z|>\frac{M(\epsilon)}{2})<\frac{\epsilon}{2}$. Thus $\exists N\in\N, n>N\Rightarrow P(|z_n-z|>\frac{M(\epsilon)}{2})<\frac{\epsilon}{2}$.
				\item For $n\le N$, $\exists M_n>0$ s.t. $P(|z_n|>M_n)<\epsilon$
				\item Let $M_{\epsilon}=\max\{M_1,\cdots,M_N,M(\epsilon) \}$
				\item If $n\le N$, $P(|z_N|>M_{\epsilon})\le P(|z_N|>M_n)<\epsilon$\\
				If $n>N$,
				\begin{align*}
				P(|z_n|>M_{\epsilon})&\le P(|z_n-z|+|z|>M_{\epsilon})\\
				&\le P(|z_n-z|>\frac{M_{\epsilon}}{2})+P(|z|>\frac{M_{\epsilon}}{2})\\
				&\le P(|z_n-z|>\frac{M(\epsilon)}{2})+P(|z|>\frac{M(\epsilon)}{2})\\
				&<\epsilon
				\end{align*}
			\end{itemize}
		\end{proof}
		\item $z_n\dlim z \Rightarrow z_n=O_p(1)$
%		\begin{proof}
%			(Prohorov's)\\
%			Fix $M$ s.t. $P(|z|\ge M)<\epsilon$. By Portmanteau lemma, $P(||z_n||\ge M)$ exceeds $P(||z||\ge M)$ arbitarily little for sufficiently large $n$. Thus $\exists N$ s.t. $P(||z_n||\ge M)<2\epsilon,\forall n\ge N$. Because each of the finitely many variables $z_n$ with $n<N$ is tight, the value of $M$ can be increased to ensure that $P(||z_n||\ge M)<2\epsilon,\forall n$.
%		\end{proof}
		\item If $E(|z_n|)\le c<+\infty,\forall n$, then $z_n=O_p(1)$
		\begin{proof}
			Use Markov inequality. Let $\epsilon>0$. Then there exists $M(\epsilon)$ such that $\frac{c}{M(\epsilon)}<\epsilon$. Then for each $n\in\N,P(|z_n|>M(\epsilon))\le \frac{E(|z_n|)}{M(\epsilon)}\le \frac{c}{M(\epsilon)}<\epsilon$.
		\end{proof}
		\item If $z_n=O_p(1), a_n\rightarrow\infty$ then $\frac{z_n}{a_n}=o_p(1)$.
		\begin{proof}
			$\exists M_{\delta}>0$ s.t. $P(|z_n|>M_{\delta})<\delta,\forall n$. Fix $\epsilon>0$ then
			\begin{align*}
			P(|z_n/a_n|>\epsilon)&=P(|z_n/a_n|>\epsilon,|z_n|>M_{\delta})+P(|z_n/a_n|>\epsilon,|z_n|\le M_{\delta})\\
			&\le P(|z_n|>M_{\delta})+P(|M_{\delta}/a_n|>\epsilon)\\
			&<\delta
			\end{align*}
			as $\ninfty$ and since $\delta$ arbitarily chosen, we're done.
		\end{proof}
	\end{enumerate}
\end{rmk}

\begin{thm}
	\textbf{(Random Sequence with a Bounded Moment is Stochastically Bounded)}\\
	If $z_n$ is a random vector which satisfies
	\begin{align*}
	E||z_n||^{\delta}=O(a_n)
	\end{align*}
	for some sequence $a_n$ and $\delta>0$, then
	\begin{align*}
	z_n=O_p(a_n^{1/\delta})
	\end{align*}
	Similarly, $E||z_n||^{\delta}=o(a_n)$ implies $z_n=o_p(a_n^{1/\delta}).$
	
	\begin{proof}
		The assumptions imply that there is some $M<+\infty$ such that $E||z_n||^{\delta}\le Ma_n$ for all $n$. For any $\epsilon$ set $B=(\frac{M}{\epsilon})^{1/\delta}$. Then
		\begin{align*}
		P(a_n^{-1/\delta}||z_n||>B)=P(||z_n||^{\delta}>\frac{Ma_n}{\epsilon})\le\frac{\epsilon}{Ma_n}E||z_n||^{\delta}\le\epsilon
		\end{align*}
	\end{proof}
\end{thm}

\begin{thm}\label{thm:simplerulesforstochasticorders}
	\textbf{(Simple Rules for Stochastic Orders)}
	\begin{enumerate}
		\item $o_p(1)+o_p(1)=o_p(1)$
		\item $o_p(1)+O_p(1)=O_p(1)$
		\item $O_p(1)+O_p(1)=O_p(1)$
		\item $o_p(1)o_p(1)=o_p(1)$
		\item $o_p(1)O_p(1)=o_p(1)$
		\item $O_p(1)O_p(1)=O_p(1)$
	\end{enumerate}
	\begin{proof}\leavevmode
		Below I provide proof for some of the above. Rest of them can be proved using Continuous Mapping theorem and Slutsky's theorem.
		\begin{itemize}
			\item[3.] Let $y_n=O_p(1),z_n=O_p(1).$ Fix $\epsilon>0$. Then $\exists M_y>0$ s.t. $P(|y_n|>M_y)<\frac{\epsilon}{2},\forall n$, $\exists M_z>0$ s.t. $P(|z_n|>M_z)<\frac{\epsilon}{2},\forall n$. Let $M_{\epsilon}=M_y+M_z$ then
			\begin{align*}
			P(|z_n+y_n|>M_{\epsilon})&\le P(|z_n|+|y_n|>M_{\epsilon})\\
			&\le P(|z_n|>M_z)+P(|y_n|>M_y)<\epsilon
			\end{align*}
			
			\item[5.] Let $y_n=o_p(1),z_n=O_p(1).$ Fix $\epsilon,\delta>0$. Let $M_{\epsilon}$ be s.t. $P(|z_n|>M_{\epsilon})<\frac{\epsilon}{2},\forall n$. For sufficiently large $n$,
			\begin{align*}
			P(|z_ny_n|>\delta)&=P(|z_ny_n|>\delta,|z_n>M_{\epsilon})+P(|z_ny_n|>\delta,|z_n|\le M_{\epsilon})\\
			&\le P(|z_n|>M_{\epsilon})+P(|y_n|>\frac{\delta}{M_{\epsilon}})\\
			&\le \epsilon
			\end{align*}
			as $\ninfty$ since $y_n=o_p(1)$.
			
			\item[6.] Let $y_n=O_p(1),z_n=O_p(1).$ Fix $\epsilon>0$. Then $\exists M_y>0$ s.t. $P(|y_n|>M_y)<\frac{\epsilon}{2},\forall n$, $\exists M_z>0$ s.t. $P(|z_n|>M_z)<\frac{\epsilon}{2},\forall n$. Let $M_{\epsilon}=M_y\cdot M_z$ then,
			\begin{align*}
			P(|z_ny_n|>M_{\epsilon})&=P(|z_n||y_n|>M_{\epsilon})\\
			&\le P(|z_n|>M_{\epsilon})+P(|y_n|>M_y)\\
			&<\epsilon
			\end{align*}
		\end{itemize}
	\end{proof}
\end{thm}
\begin{ex}
	\textbf{(Example of using Theorem \ref{thm:simplerulesforstochasticorders})}\\
	$\sqrt{n}(\bhat-\beta)\dlim N(0,1)$ implies $\bhat=\beta+o_p(1)$ so that $\bhat$ is a consistent estimator of $\beta$.
	\begin{proof}
		\begin{align*}
		\bhat-\beta=\frac{1}{\sqrt{n}}\cdot\sqrt{n}(\bhat-\beta)=\frac{1}{\sqrt{n}}\cdot O_p(1)=o_p(1)\cdot O_p(1)=o_p(1)
		\end{align*}
	\end{proof}
\end{ex}


\subsection{Inequalities\footnote{In this subsection I restate a number of useful equalities and their proofs from Hansen's textbook. You don't have to know the proofs unless you are interested in.}}
\begin{thm}\label{thm:jensensineq}
	\textbf{(Jensen's Inequality)}\\
	If $g(\cdot):\R^m\rightarrow\R$ is convex, then for any random vector $x$ for which $E||x||<+\infty$ and $E|g(x)|<+\infty$,
	\begin{align*}
	g(E(x))\le E(g(x)).
	\end{align*}
	\begin{proof}
		Since $g(u)$ is convex, at any point $u$ there is a nonempty set of subderivatives (linear surfaces touching $g(u)$ at $u$ but lying below $g(u)$ for all $u$). Let $a+b^t u$ be a subderivative of $g(u)$ at $u=E(x)$. Then for all $u$, $g(u)\ge a+b^tu$ yet $g(E(x))=a+b^TE(x)$. Applying expectations, $E(g(x))\ge a+b^T E(x)=g(E(x))$.
	\end{proof}
\end{thm}

\begin{thm}\label{thm:conditionaljensensineq}
	\textbf{(Conditional Jensen's Inequality)}\\
	If $g(\cdot):\R^m\rightarrow\R$ is convex, then for any random vectors $(y,x)$ for which $E||y||<+\infty$ and $E||g(y)||<+\infty$,
	\begin{align*}
	g(E(y|x))\le E(g(y)|x)
	\end{align*}	
\end{thm}

\begin{thm}\label{thm:conditionalexpectationineq}
	\textbf{(Conditional Expectation Inequality)}\\
	For any $r\ge 1$ such that $E|y|^r<+\infty$, then
	\begin{align*}
	E|E(y|x)|^r\le E|y|^r <+\infty
	\end{align*}
	
	\begin{proof}
		By Conditional Jensen's inequality and the law of iterated expectations.
	\end{proof}	
\end{thm}

\begin{thm}\label{thm:expectationineq}
	\textbf{(Expectation Inequality)}\\
	For any random matrix $Y$ for which $E||Y||<+\infty$,
	\begin{align*}
	||E(Y)||\le E||Y||.
	\end{align*}
	\begin{proof}
		Since matrix norm $||\cdot||$ is convex, apply Jensen's inequality.
	\end{proof}
\end{thm}


\begin{thm}\label{thm:holdersineq}
	\textbf{(H\"{o}lder's Inequality)}\\
	If $p>1$ and $q>1$ and $\frac{1}{p}+\frac{1}{q}=1$, then for any random $m\times n$ matrices $X$ and $Y$,
	\begin{align*}
	E||X^TY||\le(E||X||^p)^{1/p}(E||Y||^q)^{1/q}.
	\end{align*}
	\begin{proof}
		Since $\frac{1}{p}+\frac{1}{q}=1$, $exp(\cdot)$ is convex, apply Jensen's Inequality. For any real $a$ and $b$,
		\begin{align*}
		\exp\Big[\frac{1}{p}a+\frac{1}{q}b\Big]\le\frac{1}{p}\exp(a)+\frac{1}{q}\exp(b)
		\end{align*}
		Now let $u=exp(a)$ and $v=exp(b)$. Then,
		\begin{align*}
		u^{1/p}v^{1/q}\le\frac{u}{p}+\frac{v}{q}
		\end{align*}
		Now let $u=||X||^p/E||X||^p$ and $v=||Y||^q/E||Y||^q$. Note that $E(u)=E(v)=1$. By matrix Schwarz Inequality, $||X^TY||\le||X||||Y||$. Thus,
		\begin{align*}
		\frac{E||X^TY||}{(E||X||^p)^{1/p}(E||Y||^q)^{1/q}}&\le\frac{E(||X||||Y||)}{(E||X||^p)^{1/p}(E||Y||^q)^{1/q}}\\
		&=E(u^{1/p}v^{1/q})\\
		&\le E(\frac{u}{p}+\frac{v}{p})\\
		&=\frac{1}{p}+\frac{1}{q}\\
		&=1
		\end{align*}
	\end{proof}
\end{thm}

\begin{thm}\label{thm:CSineq}
	\textbf{(Cauchy-Schwarz Inequality)}\\
	For any random $m\times n$ matrices $X$ and $Y$,
	\begin{align*}
	E||X^TY||\le (E||X||^2)^{1/2}(E||Y||^2)^{1/2}
	\end{align*}
\end{thm}

%\begin{thm}
%	\textbf{(Matrix Cauchy-Schwarz Inequality)}\\
%	Triathi (1999). For any random $x\in\R^m$ and $y\in\R^l$,
%	\begin{align*}
%	E(yx^T)(E(xx^T))^-E(xy^T)\le E(yy^T)
%	\end{align*}	
%\end{thm}

\begin{thm}\label{thm:minkowski'sineq}
	\textbf{(Minkowski's Inequality)}\\
	For any random $m\times n$ matrices $X$ and $Y$,
	\begin{align*}
	(E||X+Y||^p)^{1/p}\le(E||X||^p)^{1/p}+(E||Y||^p)^{1/p}
	\end{align*}	
	\begin{proof}
		\begin{align*}
		E||X+Y||^p&=E(||X+Y||||X+Y||^{p-1})\\
		&\le E(||X||||X+Y||^{p-1})+E(||Y||||X+Y||^{p-1})\\
		&\le (E||X||^p)^{1/p}E(||X+Y||^{q(p-1)})^{1/q}\\
		&+(E||Y||^p)^{1/p}E(||X+Y||^{q(p-1)})^{1/q}\\
		&=((E||X||^p)^{1/p}+(E||Y||^p)^{1/p})E(||X+Y||^p)^{(p-1)/p}
		\end{align*}
		Divide both sides by $E(||X+Y||^p)^{(p-1)/p}.$
	\end{proof}
\end{thm}

\begin{thm}\label{thm:liapunovsineq}
	\textbf{(Liapunov's Inequality)}\\
	For any random $m \times n$ matrix $X$ and $1\le r\le p$,
	\begin{align*}
	(E||X||^r)^{1/r}\le (E||X||^p)^{1/p}
	\end{align*}	
	\begin{proof}
		Note that function $g(u)=u^{p/r}$ is convex for $u>0$ since $p\ge r$. Let $u=||X||^r$ and apply Jensen's inequality.
	\end{proof}
\end{thm}

\begin{thm}\label{thm:markoveineq}
	\textbf{(Markov Inequality Standard Form)}\\
	For any random vector $x$ and non-negative function $g(x)\ge 0$,
	\begin{align*}
	P(g(x)>\alpha)\le\frac{E(g(x))}{\alpha}
	\end{align*}
\end{thm}

\begin{thm}\label{thm:markovineqstrong}
	\textbf{(Markov Inequality Strong Form)}\\
	For any random vector $x$ and non-negative function $g(x)\ge 0$,
	\begin{align*}
	P(g(x)>\alpha)\le\frac{E(g(x)1(g(x)>\alpha))}{\alpha}.
	\end{align*}
	
	\begin{proof}
		Let $F$ denote the distribution function of x. Then
		\begin{align*}
		P(g(x)\ge\alpha)&=\int_{\{g(u)\ge\alpha\}}dF(u)\\
		&\le\int_{\{g(u)\ge\alpha\}}\frac{g(u)}{\alpha}dF(u)\\
		&=\alpha^{-1}\int 1(g(u)>\alpha)g(u)dF(u)\\
		&=\alpha^{-1} E(g(x)1(g(x)>\alpha))
		\end{align*}
		the inequality using the region of integration $\{g(u)>\alpha\}$. Since $1(g(x)>\alpha)\le 1$, the final expression is less than $\frac{E(g(x))}{\alpha}$, establishing the standard form.
	\end{proof}
\end{thm}

\begin{thm}\label{thm:chebyshevineq}
	\textbf{(Chebyshev's Inequality)}\\
	For any random variable $x$,
	\begin{align*}
	P(|x-E(x)|>\alpha)\le\frac{Var(x)}{\alpha^2}
	\end{align*}
	
	\begin{proof}
		Define $y=(x-E(x))^2$ and note that $E(y)=Var(x)$. The events $\{|x-E(x)|>\alpha\}$ and $\{y>\alpha^2\}$ are equal, so by an application Markov's inequality we find
		\begin{align*}
		P(|x-E(x)|>\alpha)=P(y>\alpha^2)\le\alpha^{-2}E(y)=\frac{Var(x)}{\alpha^2}
		\end{align*}
	\end{proof}
\end{thm}
\clearpage

\section{Instrumental Variable}
\subsection{Endogeneity}
Consider a linear model
\begin{align}\label{eqn:IVstructural}
y_i=x_i'\beta+e_i
\end{align}
Among others, the key assumption for consistency of $\bhat$ was $E(x_ie_i)=0$ since
\begin{align*}
\bhat&\plim E(x_ix_i')^{-1}E(x_iy_i)\\
     &= \beta+\underbrace{E(x_ix_i')^{-1}E(x_ie_i)}_{\text{endogeneity bias if }E(x_ie_i)\neq0}
\end{align*}
We call there is an endogeneity in the linear model if $E(x_ie_i)=0$ is violated.

\begin{defn}
	\textbf{(Endogeneity)}\\
	In a linear model $y_i=x_i'\beta+e_i$, we say that there is \textbf{endogeneity} in the linear model if $\beta$ is the parameter of interest and
	\begin{align*}
	E(x_ie_i)\neq 0
	\end{align*}
	It is typical to say that $x_i$ is endogenous for $\beta$.
\end{defn}

It turns out that for endogeneity, $\beta_0$ must describe more than just linear projection or conditional expectation. For example, suppose given i.i.d. data $\{y_i,x_i\}_{i=1}^n$, define
\begin{align*}
\beta^*&=E(x_ix_i')^{-1}E(x_iy_i)\\
u_i^*&=y_i-x_i'\beta^*
\end{align*}
So that $\beta^*$ is the linear projection coefficient and $u_i^*$ is the linear projection error. Then it is clear that
\begin{align*}
y_i=x_i'\beta^*+u_i^*\text{ with }E(u_i^*x_i)=0
\end{align*}
But still, 
\begin{align*}
\beta^*&=E(x_ix_i')^{-1}E(x_iy_i)\\
   	   &=E(x_ix_i')^{-1}E(x_i(x_i'\beta+e_i))\\
   	   &=\beta+E(x_ix_i)^{-1}E(x_ie_i)
\end{align*}
and it is not equal to $\beta$ if endogeneity is present.

As another example, suppose given i.i.d. data $\{y_i,x_i\}_{i=1}^n$, define
\begin{align*}
E(y_i|x_i)&=x_i'\beta^*\\
u_i^*&=y_i-x_i'\beta^*
\end{align*}
then it is clear that
\begin{align*}
y_i=x_i'\beta^*+u_i^*\text{ with }E(u_i^*|x_i)=0
\end{align*}
and $E(u_i^*x_i)=E(E(u_i^*x_i|x_i))=E(E(u_i^*|x_i)x_i)=0$.

Hence to distinguish Equation (\ref{eqn:IVstructural}) from linear projection or conditional models, we call Equation (\ref{eqn:IVstructural}) a \textbf{structural equation} and $\beta$ a \textbf{structural parameter}.

In most cases, $\beta_0$ in a model with endogeneity is an interpretable parameter in an idealized model that cannot be estimated due to practical limitations. 

\subsection{Sources of Endogeneity}
Three major examples of sources of endogeneity are omitted variable bias, measurement error and simultaneous equations.

\subsubsection{Omitted Variable Bias\footnote{Though it is a convention to write as omitted variable ``bias", one should be aware that the bias here is an asymptotic concept.}}
Suppose that we want to estimate the linear model
\begin{align*}
y_i=x_i'\beta+w_i'\delta+e_i\text{ with }E(e_i(x_i',w_i')')=0
\end{align*}
However, we do not observe $(y_i,x_i,w_i)$ but only $(y_i,x_i)$. That is, by defining
\begin{align*}
u_i=w_i'\delta+e_i
\end{align*}
we can only at best estimate the model
\begin{align*}
y_i=x_i'\beta+u_i
\end{align*}
Then there is endogeneity of $x_i$ for $\beta$ if $x_i$ and $w_i$ are correlated and $\delta\neq0$ since
\begin{align*}
E(x_iu_i)=E(x_iw_i')\delta\neq0
\end{align*}
If $x_i$ and $w_i$ are uncorrelated, basically we cannot distinguish between $w_i$ and $e_i$ so there is no problem. 

As a consequence, the probability limit of the OLS estimator that only uses $\{y_i,x_i\}_{i=1}^n$ is 
\begin{align*}
\beta^*&=\beta+E(x_ix_i')^{-1}E(x_iw_i')\delta\\
&\neq \beta
\end{align*}
so the OLS estimator is inconsistent. 

The direction of the (asymptotic) bias depends on the sign of $E(x_iw_i')$ and $\delta$, which we can guess in some applications. If $E(x_iw_i')\delta$ is positive, we call that $\bhat$ is \textbf{over-estimated} and if $E(x_iw_i')\delta$ is negative, we call that $\bhat$ is \textbf{under-estimated}. 
Especially in the case where parameter estimates are to be given a causal interpretation, controlling for omitted variables bias is necessary. Since too many regressors cause little harm, but too few regressors can lead to inconsistency, microeconometric models estimated from large data sets tend to include many regressors.

\begin{ex}\label{ex:ovbexample}
	Suppose wage $y_i$ is determined by education $x_i$ and unobserved ability $w_i$ as $y_i=x_i\beta+w_i\delta+e_i,E(e_i(x_i,w_i)')=0$ but at best we can only estimate the model $y_i=x_i\beta+u_i$. If wages are affected by unobserved ability ($\delta\neq0$) and individuals with high ability self-select into higher education ($E(x_iw_i')\neq0$) then the error term contains unobserved ability ($u_i=w_i\delta+e_i$) so education and error will be positively correlated ($E(u_ix_i)\neq0$). Hence we have education $x_i$ endogenous for $\beta$ and th e linear projection coefficient will be upward biased relative to the structural coefficient $\beta$.
\end{ex}

\begin{rmk}
	\textbf{(Inclusion of Irrelevant Regressors)}\\Related form of misspecification is inclusion of irrelevant regressors. In this case it is straightforward to show that OLS is consistent, but there is a loss of efficiency. 
\end{rmk}

\subsubsection{Measurement Error}
Suppose that we want to estimate the linear model
\begin{align*}
y_i=x_i^{*\prime}\beta+e_i\text{ with }E(e_i x_i^*)=0
\end{align*}
However, we do not observe $x_i^*$ but only $x_i=x_i^*+v_i$ with $v_i$ a mean zero measurement error that is independent of $x_i^*$ and $e_i$. This case is known as \textbf{classical measurement error}, which means $x_i$ is a noisy but unbiased measure of $z_i$.

Define $u_i=e_i-v_i'\beta$, we find that
\begin{align*}
y_i&=x_i^{*\prime}\beta+e_i\\
&=(x_i-v_i)'\beta+e_i\\
&=x_i'\beta+u_i
\end{align*}
Then there is endogeneity of $x_i$ for $\beta$ because
\begin{align*}
E(x_iu_i)=E((x_i^*+v_i)(e_i-v_i'\beta_0))=-E(v_iv_i')\beta\neq 0
\end{align*}
in general.

As a consequence, the probability limit of the OLS estimator that uses only $\{y_i,x_i\}_{i=1}^n$ is
\begin{align*}
\beta^* &=\beta-E(x_ix_i')^{-1}E(v_iv_i')\beta\\
&=\beta-[E(x_i^*x_i^{* \prime}) +E(v_iv_i')]^{-1}E(v_iv_i^{\prime})\beta\tag*{: $x_i^*,v_i$ independent}\\
&=[E(x_i^*x_i^{*\prime})+E(v_iv_i')]^{-1}E(x_i^*x_i^{*\prime})\beta
\end{align*}
So OLS estimator is again inconsistent. Suppose that $x_i$ and $x_i^*$ are both scalar random variables. Then we find the probability limit of OLS estimator as
\begin{align*}
\underbrace{\frac{E(x_i^*x_i^{*\prime})}{E(x_i^*x_i^{*\prime})+E(v_iv_i')}}_{\in(0,1)}\beta
\end{align*}
hence the it shrinks the structural parameter $\beta$ towards zero. This is called \textbf{measurement error bias} or \textbf{attenuation bias}.

\begin{rmk}\leavevmode
	\begin{itemize}
		\item Note that additive measurement error in dependent variable $y_i$ with all assumptions above is not a problem since the measurment error just adds to the error $e_i$.
		\item If we have one more variable which is a noisy measure of $x_i^*$, say $w_i=x_i^*+\eta_i$, we can use as an instrument for $x_i$.
	\end{itemize}
\end{rmk}

\subsubsection{Simultaneous Equations}
Consider a simple linear supply-and-demand system:
\begin{align*}
q_i&=-\beta p_i+e_{1i}\tag*{: demand}\\
q_i&=\beta p_i+e_{2i}\tag*{: supply}
\end{align*}
Assume that $e_i=\begin{pmatrix}e_{1i}\\e_{2i}\end{pmatrix}$ is i.i.d., $E(e_i)=0$ and $E(e_ie_i')=I_2$\footnote{$I_k$ is an identity matrix of dimension $k$.} for simplicity.

Simple algebra shows that in equilibrium,
\begin{align*}
\begin{bmatrix}1&\beta_1\\1 &-\beta_2\end{bmatrix}\begin{pmatrix}q_i\\p_i\end{pmatrix}=\begin{pmatrix}e_{1i}\\e_{2i}\end{pmatrix}
\end{align*}
and hence
\begin{align*}
\begin{pmatrix}q_i\\p_i\end{pmatrix}&=\begin{bmatrix}1&\beta_1\\1 &-\beta_2\end{bmatrix}^{-1}\begin{pmatrix}e_{1i}\\e_{2i}\end{pmatrix}\\
&=\begin{pmatrix}
\frac{\beta_2 e_{1i}+\beta_1 e_{2i}}{\beta_1+\beta_2}\\
\frac{e_{1i}-e_{2i}}{\beta_1+\beta_2}
\end{pmatrix}
\end{align*}
Hence, in both demand and supply equations the regressor $p_i$ is correlated with the errors
\begin{align*}
E(p_ie_{1i})&\neq0\\
E(p_ie_{2i})&\neq0
\end{align*}
and hence we have endogeneity of $p_i$ for both $\beta_1$ and $\beta_2$. 
The projection of $q_i$ on $p_i$ yields
\begin{align*}
q_i&=\beta^* p_i+e_i^*\\
E(p_ie_i^*)*&=0
\end{align*}
where
\begin{align*}
\beta^*=\frac{E(p_iq_i)}{E(p_i^2)}=\frac{\beta_2-\beta_1}{2}
\end{align*}
and hence the OLS estimate $\bhat\plim \beta^*$ and it does not equal either $\beta_1$ and $\beta_2$. This is called the \textbf{simultaneous equations bias}. In general, when both the dependent variable and independent variable are simultaneously determined then they should be treated as endogenous.


\subsection{Instrumental Variables}
Again consider a linear model with endogeneity:
\begin{align*}
y_i=x_i'\beta+e_i\text{ with }E(e_ix_i)\neq 0
\end{align*}
How can we identify and estimate $\beta$? An obvious way is through a randomized experiment, but for most economics applications such experiments are too expensive or even impossible. One popular approach is through the use of so-called instruments or instrumental variables (IV). This is to generate only \textit{exogenous} variation in $x$. 

In most applications we only treat a subset of the regressors as endogenous; most of the regressors will be treated as exogenous, meaning that they are assumed uncorrelated with the equation error. To be specific, we make the partition

\begin{align*}
x_i=\begin{pmatrix}x_{1i}\\x_{2i}\end{pmatrix}\begin{matrix}k_1\\k_2\end{matrix}
\end{align*}
and similarly
\begin{align*}
\beta=\begin{pmatrix}\beta_{1}\\\beta_{2}\end{pmatrix}\begin{matrix}k_1\\k_2\end{matrix}
\end{align*}
and assume 
\begin{align*}
E(x_{1i}e_i)&=0\\
E(x_{2i}e_i)&\neq0
\end{align*}
where $k_1+k_2=k$ and so that $x_{1i}$ is \textbf{exogenous} and $x_{2i}$ is \textbf{endogenous} for the structural parameter $\beta$. The \textbf{structural equation} is
\begin{align*}
y_i&=x_i'\beta+e_i\\
   &=x_{1i}'\beta_1+x_{2i}'\beta_2+e_i
\end{align*}
or equivalently using matrix notation,
\begin{align*}
y&=X\beta+e\\
 &=X_1\beta+X_2\beta_2+e
\end{align*}

To consistently estimate $\beta$, we require additional information. One type of information which is commonly used in economic applications are what we call instruments.

\begin{defn}
	\textbf{(Instrumental Variables)}\\
	An $l$-vector $z_i$ is a vector of IVs for $x_i$ in a regression of the form
	\begin{align*}
	y_i=x_i'\beta+e_i
	\end{align*}
	if it satisfies
	\begin{enumerate}
		\item Exogeneity : $E(z_iu_0)=0$
		\item Relevance : $rank(E(z_ix_i'))=k$
		\item $E(z_iz_i')>0$
	\end{enumerate}
\end{defn}

Roughly speaking, instrument $z_i$ should be correlated with $x_i$ and uncorrelated with $u_i$. Exogeneity condition\footnote{Exogenous variables mean they are determined outside of the model for $y_i$.} requires the instruments are uncorrelated with the regression error, and the relevance condition is essential for the identification of the model and a necessary condition follows, $l\ge k$. Third one is a normalization which excludes linearly redundant instruments.

Note that even if $E(x_ie_i)\neq0$, $x_{1i}$ is still uncorrelated with $e_i$ (for example, the intercept) and they should be included as instrumental variables, as a subset of variables $z_i$. Hence,
\begin{align*}
z_i=\begin{pmatrix}z_{1i}\\z_{2i}\end{pmatrix}=\begin{pmatrix}x_{1i}\\z_{2i}\end{pmatrix}=\begin{matrix}k_1\\l_2\end{matrix}
\end{align*}
where $k_1+l_2=l$. 

Many authors simply label $x_{1i}$ as the ``exogenous variables", $x_{2i}$ as the ``endogenous variables" and $z_{2i}$ as the ``instrumental variables".

\begin{ex}
	\textbf{(Example \ref{ex:ovbexample} Continued)}\\
	An ideal instrument affects the choice of the regressor	(education) but does not directly influence the dependent variable (wages) except through the
	indirect effect on the regressor. \cite{card1993NBER} suggested if a potential student lives close to a college, this reduces the cost of attendence and thereby raises the likelihood that the student will attend college (increase education, `relevance'). However, college proximity does not directly affect a students skills or abilities, so should not have a direct effect on his or her market wage (`exogenous'). In this case college proximity can be used as instrument for education.
\end{ex}

\begin{defn}
	\textbf{(Identification)}\\
	In a model with instruments, we call the model is \textbf{just-identified} if $l=k$ (i.e., $l_2=k_2$) and is \textbf{over-identified} if $l>k$ (i.e., $l_2>k_2$). If $l<k$, the model is unidentified.
\end{defn}

\subsection{Reduced Form}



\newpage
\bibliographystyle{economet}
\bibliography{econometrics}

\end{document}
